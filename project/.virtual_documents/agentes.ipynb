get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")

import nest_asyncio

nest_asyncio.apply()


import base64
import json

from playwright.async_api import async_playwright
from src.browser import set_browser, set_context, set_page
from src.clear_html import clean_html_for_llm
from src.prompts.homepage_check import ResultadoBuscaServidores, check_homepage
from src.proxies import get_masked_proxy, get_proxy, test_proxy


from src.tools.search.providers import duckduckgo


results = await duckduckgo.search(
    "trump",
    search_type=["news", "web"],
    use_proxy=True,
    region="br-pt",
    site="reddit.com/r/brasil",
)

results





results = await duckduckgo.search(
    '"Eduardo Messias de Morais"',
    search_type=["news", "web"],
    use_proxy=True,
    region="br-pt",
    # site="reddit.com/r/brasil",
)

results











results = await duckduckgo.search(
    "setor agro tedencias de mercado",
    search_type=["web", "news"],
    use_proxy=True,
    region="br-pt",
)

results

















import asyncio

from src.browser import navigate_with_retry
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

# C√≥digo principal modificado
url = "https://descomplica.pocosdecaldas.mg.gov.br/info.php?c=68"
# url = "https://resende.rj.gov.br/blogtransparencia/page/index.asp"

proxy_config = await get_proxy()

async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="random", proxy=proxy_config)
    context = await set_context(browser)
    page = await set_page(context)

    await navigate_with_retry(page, url)

    await asyncio.sleep(3)

    html_content = await page.content()
    screenshot_bytes = await page.screenshot(full_page=True, type="jpeg", quality=40)
    screenshot_base64 = base64.b64encode(screenshot_bytes).decode("utf-8")

response = await check_homepage(url, html_content)
print(f"Custo = {response.cost_brl}")
parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)








print("=== EXEMPLOS SIMPLIFICADOS ===")

# Exemplo b√°sico
query1 = "casa comprar po√ßos de caldas"
url1 = duckduckgo.create_url(query1)
print(f"B√°sico: {url1}")
print()

# Pesquisa com aspas
query2 = 'casa comprar "po√ßos de caldas"'
url2 = duckduckgo.create_url(query2)
print(f"Com aspas: {url2}")
print()

# Pesquisa brasileira sem safe search
url3 = duckduckgo.create_url("python programming", region="br-pt", safe_search=-2)
print(f"Brasil, sem safe search: {url3}")
print()

# Pesquisa de imagens
url4 = duckduckgo.create_url(
    "beautiful landscapes", search_type="images", region="us-en"
)
print(f"Busca de imagens: {url4}")
print()

# Pesquisa de v√≠deos
url5 = duckduckgo.create_url("python tutorials", search_type="videos")
print(f"Busca de v√≠deos: {url5}")
print()

# Configura√ß√£o focada em privacidade
url6 = duckduckgo.create_url(
    "privacy tools", advertisements=-1, redirect=-1, full_urls=1
)
print(f"Foco em privacidade: {url6}")
print()

# Pesquisa de not√≠cias
url7 = duckduckgo.create_url("tech news 2025", search_type="news", region="us-en")
print(f"Busca de not√≠cias: {url7}")





# await test_proxy(proxy_config)





import asyncio

import bs4
from bs4 import BeautifulSoup
from IPython.display import HTML, Markdown, display
from src.browser import navigate_with_retry
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)


async def search(query: str, search_type: str):

    url = duckduckgo.create_url(
        query,
        search_type=search_type,
    )
    proxy_config = await get_proxy(test=False)
    html_content = await _get_search_html(url)
    articles = await _get_articles_from_html(search_type, html_content)
    return await _parse_articles(search_type, articles=articles)


def print_retry_attempt(retry_state):
    print(
        f"üîÑ Retry {retry_state.attempt_number} - Erro: {retry_state.outcome.exception()}"
    )
    print(f"‚è≥ Aguardando {retry_state.next_action.sleep:.1f} segundos...")


def print_final_result(retry_state):
    if retry_state.outcome.failed:
        print(f"‚ùå Falha final ap√≥s {retry_state.attempt_number} tentativas")
    else:
        print(f"‚úÖ Sucesso na tentativa {retry_state.attempt_number}")


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=2, min=2, max=10),
    retry=retry_if_exception_type((UnexpectedDuckDuckGoError)),
    before_sleep=print_retry_attempt,
    after=print_final_result,
    reraise=True,
)
async def _get_search_html(
    url,
    proxy_config=None,
    engine="firefox",
    timeouts=[60000, 90000, 120000],
    wait_time=0,
):

    async with async_playwright() as playwright:
        browser = await set_browser(playwright, engine=engine, proxy=proxy_config)
        context = await set_context(browser)
        page = await set_page(context)

        strategies = ["networkidle", "domcontentloaded", "load"]
        await navigate_with_retry(
            page, url, timeouts=timeouts, strategy_priority=strategies
        )

        await asyncio.sleep(wait_time)

        html_content = await page.content()

        if "Unexpected error" in html_content:
            raise UnexpectedDuckDuckGoError("Unexpected DuckDuckGo Error.")

        return html_content


async def _parse_web_articles(articles: list[bs4.element.Tag]) -> list[dict[str, str]]:
    results = []
    for article in articles:

        try:
            links = article.find_all("a", {"data-testid": "result-extras-url-link"})
            links = [link.find("div").text for link in links]

            titles = article.find_all("a", {"data-testid": "result-title-a"})
            titles = [title.find("span").text for title in titles]

            snippets = article.find_all("div", {"data-result": "snippet"})
            snippets = [snippet.find("span").text for snippet in snippets]

            result = [
                {"link": l, "title": t, "snippet": s}
                for l, t, s in zip(links, titles, snippets)
            ]

            results.extend(result)
        except Exception as err:
            raise ParseWebArticleError(article)

    return results


async def _parse_news_articles(articles: list[bs4.element.Tag]) -> list[dict[str, str]]:
    results = []
    for article in articles:

        try:
            tags_a = article.find_all("a")
            link = tags_a[0].get("href")

            tags_h2 = article.find_all("h2")
            title = tags_h2[0].text

            tags_span = article.find_all("span")
            source = tags_span[0].text

            divs_with_text = [
                div
                for div in article.find_all("div")
                if not div.find_all() and div.get_text(strip=True)
            ]
            times = [div.text for div in divs_with_text if "ago" in str(div)]
            relative_time = times[0]

            tags_p = article.find_all("p")
            snippet = tags_p[0].text

            result = {
                "link": link,
                "title": title,
                "source": source,
                "snippet": snippet,
                "relative_time": relative_time,
            }

            results.append(result)
        except Exception as err:
            raise ParseNewsArticleError(article)

    return results


async def _parse_articles(
    search_type: str, articles: list[bs4.element.Tag]
) -> list[dict[str, str]]:
    match search_type:
        case "web":
            return await _parse_web_articles(articles)
        case "news":
            return await _parse_news_articles(articles)
        case "videos":
            raise NotImplementedError(f"Search of videos is not implemented yet.")
        case "images":
            raise NotImplementedError(f"Search of images is not implemented yet.")
        case _:
            raise ValueError(f"Search of '{search_type}' not recognized.")


async def _get_articles_from_html(
    search_type: str, html_content: str
) -> list[bs4.element.Tag]:
    clean_html = (await clean_html_for_llm(html_content))["cleaned_html"]
    soup = BeautifulSoup(clean_html, "html.parser")

    match search_type:
        case "web":
            return soup.find_all("article")
        case "news":
            return [
                article
                for article in soup.find_all("article")
                if not len(article.find_all("article"))
            ]
        case "videos":
            raise NotImplementedError(f"Search of videos is not implemented yet.")
        case "images":
            raise NotImplementedError(f"Search of images is not implemented yet.")
        case _:
            raise ValueError(f"Search of '{search_type}' not recognized.")


class UnexpectedDuckDuckGoError(Exception):
    pass


class ParseWebArticleError(Exception):
    def __init__(self, article: bs4.element.Tag):
        message = f"It was not possible to parse article \n```{article}```"
        super().__init__(message)


class ParseNewsArticleError(Exception):
    def __init__(self, article: bs4.element.Tag):
        message = f"It was not possible to parse article \n```{article}```"
        super().__init__(message)





# await _parse_articles(search_type="news", articles=articles)








from src.tools.search.providers.duckduckgo import (
    _get_articles_from_html,
    _get_search_html,
)

search_type = "web"  # ('web', 'images', 'videos', 'news')


url = duckduckgo.create_url(
    '"alcoa" corrup√ß√£o',
    search_type=search_type,
)
print(url)


proxy_config = await get_proxy(test=False)

html_content = await _get_search_html(url)

# articles = await _get_articles_from_html(search_type, html_content)

# await _parse_articles(search_type, articles=articles)


from bs4 import BeautifulSoup

clean_html = (await clean_html_for_llm(html_content))["cleaned_html"]
soup = BeautifulSoup(clean_html, "html.parser")
articles = soup.find_all("article")


print(article.prettify())


article = articles[4]
[
    p.text
    for d1 in article.find_all("div")
    for d2 in d1.find_all("div")
    for d3 in d2.find_all("div")
    for p in d3.find_all("p")
    if not len(p.find_all("span"))
]








search_type = "news"  # ('web', 'images', 'videos', 'news')


url = duckduckgo.create_url(
    '"alcoa" corrup√ß√£o',
    search_type=search_type,
)
print(url)


proxy_config = await get_proxy(test=False)

html_content = await _get_search_html(url)

articles = await _get_articles_from_html(search_type, html_content)

await _parse_articles(search_type, articles=articles)


search_type = "news"  # ('web', 'images', 'videos', 'news')


url = duckduckgo.create_url(
    "preta gil",
    search_type=search_type,
)
print(url)


proxy_config = await get_proxy(test=False)

html_content = await _get_search_html(url)

articles = await _get_articles_from_html(search_type, html_content)

await _parse_articles(search_type, articles=articles)


# from IPython.display import HTML, Markdown

# display(HTML(html_content))


from bs4 import BeautifulSoup
from IPython.display import HTML, Markdown, display

clean_html = (await clean_html_for_llm(html_content))["cleaned_html"]
soup = BeautifulSoup(clean_html, "html.parser")
articles = soup.find_all("article")

await _parse_web_articles(articles)


with open("teste.html", "w") as f:
    f.write(html_content)


# print(html_content)





search_type = "news"  # ('web', 'images', 'videos', 'news')


url = duckduckgo.create_url(
    '"alcoa" corrup√ß√£o',
    search_type=search_type,
    region="br-pt",
    full_urls=1,
    advertisements=-1,
)
print(url)


proxy_config = await get_proxy(test=False)


html_content = await _get_search_html(url)


# from IPython.display import HTML, Markdown

# display(HTML(html_content))





from bs4 import BeautifulSoup
from IPython.display import HTML, Markdown, display

clean_html = (await clean_html_for_llm(html_content))["cleaned_html"]
soup = BeautifulSoup(clean_html, "html.parser")
articles = [
    article
    for article in soup.find_all("article")
    if not len(article.find_all("article"))
]
# display(HTML(str(articles[0])))





await _parse_news_articles(articles)





snippet


print(article.prettify())


























links = article.find_all("a", {"data-testid": "result-extras-site-search-link"})

links


type(articles[0])








data - result = ""


# print(article.prettify())


get_ipython().getoutput("pip install html2text -qqq")


# import html2text
# import ipywidgets as widgets
# from IPython.display import HTML, Markdown, display

# h = html2text.HTML2Text()
# h.ignore_links = False
# markdown = h.handle(str(article))

# print(markdown)














# from IPython.display import HTML, Markdown

# display(HTML(html_content))





import html2text
import ipywidgets as widgets
from IPython.display import HTML, Markdown, display

h = html2text.HTML2Text()
h.ignore_links = False
markdown = h.handle(str(article))

Markdown(markdown)


print(article)





article = soup.find_all("article")[1]
display(HTML(str(article)))


import json
from urllib.parse import quote

import requests


def consultar_duckduckgo(
    query, formato="json", no_redirect=True, no_html=True, skip_disambig=True
):
    """
    Consulta a API Instant Answer do DuckDuckGo

    Par√¢metros:
    - query: termo de busca
    - formato: formato da resposta ('json' √© o padr√£o)
    - no_redirect: evita redirecionamentos
    - no_html: remove HTML da resposta
    - skip_disambig: pula p√°ginas de desambigua√ß√£o
    """

    # URL base da API do DuckDuckGo
    base_url = "https://api.duckduckgo.com/"

    # Par√¢metros da consulta
    params = {
        "q": query,
        "format": formato,
        "no_redirect": "1" if no_redirect else "0",
        "no_html": "1" if no_html else "0",
        "skip_disambig": "1" if skip_disambig else "0",
    }

    try:
        # Fazer a requisi√ß√£o
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()  # Levanta exce√ß√£o para status HTTP ruins

        # Retornar resposta JSON
        return response.json()

    except requests.exceptions.RequestException as e:
        print(f"Erro na requisi√ß√£o: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"Erro ao decodificar JSON: {e}")
        return None


get_ipython().getoutput("pip install ddgs -U -qqq")

# def busca_com_duckduckgo_search():
#     """
#     Exemplo usando a biblioteca duckduckgo-search
#     Instale com: pip install duckduckgo-search
#     """
#     try:
#         from duckduckgo_search import DDGS

#         with DDGS() as ddgs:
#             # Busca web
#             resultados = list(ddgs.text("Python programming", max_results=5))

#             print("Resultados da busca web:")
#             for i, resultado in enumerate(resultados, 1):
#                 print(f"{i}. {resultado['title']}")
#                 print(f"   {resultado['body'][:100]}...")
#                 print(f"   URL: {resultado['href']}\n")

#     except ImportError:
#         print("Para usar esta fun√ß√£o, instale: pip install duckduckgo-search")
#     except Exception as e:
#         print(f"Erro: {e}")


busca_com_duckduckgo_search()


# soup = BeautifulSoup(str(article), 'html.parser')
# html_formatado = soup.prettify()
# print(html_formatado)


# from bs4 import BeautifulSoup
# from src.clear_html import clean_html_for_llm


# clean_html = clean_html_for_llm(html_content)['cleaned_html']
# soup = BeautifulSoup(clean_html, 'html.parser')
# html_formatado = soup.prettify()

# print(html_formatado)


# print(clean_html)


html_content


import asyncio

from src.browser import navigate_with_retry

url = duckduckgo.create_url(
    "beautiful landscapes", search_type="images", region="us-en"
)

proxy_config = get_proxy()
assert await test_proxy(proxy_config)

async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="random", proxy=proxy_config)
    context = await set_context(browser)
    page = await set_page(context)

    await navigate_with_retry(page, url)

    await asyncio.sleep(3)

    html_content = await page.content()
#     screenshot_bytes = await page.screenshot(full_page=True, type="jpeg", quality=40)
#     screenshot_base64 = base64.b64encode(screenshot_bytes).decode("utf-8")

# response = await check_homepage(url, html_content)
# print(f"Custo = {response.cost_brl}")
# parsed_response = ResultadoBuscaServidores(
#     **json.loads(response.choices[0].message.content)
# )
# display(HTML(html_content))





with open("teste.html", "w") as f:
    f.write(html_content)


get_ipython().getoutput("pip install html2text -qqq")





import html2text
import ipywidgets as widgets
from IPython.display import HTML, Markdown, display

h = html2text.HTML2Text()
h.ignore_links = False
markdown = h.handle(html_content)

# Markdown(markdown)


# HTML que muda baseado em input
def mostrar_html(texto):
    display(HTML(html_content))


# Widget interativo
text_input = widgets.Text(description="Digite algo:")
widgets.interact(mostrar_html, texto=text_input)


url




















import json

print(parsed_response.model_dump_json(indent=4))


response = await check_homepage(url, html_content, model="gpt-4o")
print(f"Custo = {response.cost_brl}")
parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)
print(parsed_response.model_dump_json(indent=4))


response = await check_homepage(url, html_content, model="gemini/gemini-1.5-flash")
print(f"Custo = {response.cost_brl}")
parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)


parsed_response


response = await check_homepage(url, html_content, model="gemini/gemini-2.0-flash")
print(f"Custo = {response.cost_brl}")
parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)


response = await check_homepage(url, html_content, model="gemini/gemini-2.5-flash")
print(f"Custo = {response.cost_brl}")
parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)


response = await check_homepage(url, html_content, model="gemini/gemini-2.5-pro")
print(f"Custo = {response.cost_brl}")
parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)











import base64
import io

import google.generativeai as genai
from PIL import Image


def analisar_imagem_base64(base64_string, prompt):
    image_bytes = base64.b64decode(base64_string)
    image = Image.open(io.BytesIO(image_bytes))
    model = genai.GenerativeModel("gemini-2.5-pro")
    response = model.generate_content([prompt, image])
    return response.text


analisar_imagem_base64(screenshot_base64, "qual a cor predominante?")





import os

import litellm


def analisar_imagem_litellm(
    base64_string,
    prompt="Descreva esta imagem",
    brl_currency: float = 5.6,
    model: str = "gemini/gemini-2.5-flash",
):
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_string}"},
                },
            ],
        }
    ]

    response = litellm.completion(model=model, messages=messages, max_tokens=1000)

    cost = litellm.completion_cost(model=model, completion_response=response)

    print(f"Custo BRL = R$ {brl_currency * cost}")

    return response.choices[0].message.content


analisar_imagem_litellm(screenshot_base64, "qual a cor predominante?")


analisar_imagem_litellm(
    screenshot_base64, "qual a cor predominante?", model="gemini/gemini-2.5-pro"
)


from src.scrape_tools import show_base64

show_base64(screenshot_base64)


response = await check_homepage(
    url,
    html_content,
    model="deepseek/deepseek-chat",
    response_format={"type": "json_object"},
)
print(f"Custo = {response.cost_brl}")
parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)
print(parsed_response)

from src.scrape_tools import show_base64

show_base64(screenshot_base64)


json.loads(response.choices[0].message.content)


parsed_response.links_encontrados[0].justificativa


response = await check_homepage(url, html_content)
print(f"Custo = {response.cost_brl}")

parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)


parsed_response


url = "https://descomplica.pocosdecaldas.mg.gov.br/info.php?c=68"


proxy_config = get_proxy()

assert await test_proxy(proxy_config)

async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="random", proxy=proxy_config)
    context = await set_context(browser)
    page = await set_page(context)
    await page.goto(url, wait_until="networkidle", timeout=60000)
    html_content = await page.content()

    screenshot_bytes = await page.screenshot(full_page=True, type="jpeg", quality=40)

    screenshot_base64 = base64.b64encode(screenshot_bytes).decode("utf-8")

response = await check_homepage(url, html_content)
print(f"Custo = {response.cost_brl}")

parsed_response = ResultadoBuscaServidores(
    **json.loads(response.choices[0].message.content)
)
print(f"Parecer: {parsed_response.parecer}")


with open("teste.html", "w") as f:
    f.write(html_content)


from src.scrape_tools import show_base64

show_base64(screenshot_base64)


response = await check_homepage(url, html_content, model="deepseek/deepseek-chat")
print(f"Custo = {response.cost_brl}")

# parsed_response = ResultadoBuscaServidores(**json.loads(response.choices[0].message.content))
# print(f"Parecer: {parsed_response.parecer}")





url = "https://descomplica.pocosdecaldas.mg.gov.br/info.php?c=26"
# url = "https://resende.rj.gov.br/blogtransparencia/page/index.asp"

proxy_config = get_proxy()

assert await test_proxy(proxy_config)

async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="random", proxy=proxy_config)
    context = await set_context(browser)
    page = await set_page(context)
    await page.goto(url, wait_until="networkidle")
    html_content = await page.content()

    screenshot_bytes = await page.screenshot(full_page=True, type="jpeg", quality=40)

    screenshot_base64 = base64.b64encode(screenshot_bytes).decode("utf-8")


response = await check_homepage(url, html_content)


ResultadoBuscaServidores(**json.loads(response.choices[0].message.content))








import asyncio
import os
import random
import time
from functools import reduce
from typing import Any, Dict, List, Literal

from fake_useragent import UserAgent
from playwright.async_api import async_playwright


import pandas





import asyncio

from src.browser import navigate_with_retry

url = "https://transparencia.betha.cloud/#/"

proxy_config = get_proxy()
assert await test_proxy(proxy_config)

responses_list = []


def handle_response(response):
    condition = (
        (response.status == 200)
        & (response.url[-3:] != ".js")
        & (response.url[-4:] != ".css")
        & (response.url[-4:] != ".ttf")
        & (response.url[-4:] != ".svg")
        & (response.url[-4:] != ".png")
        & (response.url[-4:] != ".jpg")
        & (".js?" not in response.url)
        & (".woff" not in response.url)
    )

    if condition:
        responses_list.append(response)


async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="firefox", proxy=proxy_config)
    context = await set_context(browser)
    page = await set_page(context)

    page.on("response", handle_response)

    await navigate_with_retry(page, url)


responses_list


def handle_response(response):
    condition = (
        (response.status == 200)
        & (response.url[-3:] != ".js")
        & (response.url[-4:] != ".css")
        & (response.url[-4:] != ".ttf")
        & (response.url[-4:] != ".svg")
        & (response.url[-4:] != ".png")
        & (response.url[-4:] != ".jpg")
        & (".js?" not in response.url)
        & (".woff" not in response.url)
    )

    if condition:
        responses_list.append(response)





from src.proxies import get_masked_proxy, get_proxy, test_proxy

proxy = get_proxy()
print(get_masked_proxy(proxy))
await test_proxy(proxy)





import litellm

response = await litellm.acompletion(
    model="gpt-4o-mini",
    response_format={"type": "json_object"},
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant designed to output JSON.",
        },
        {"role": "user", "content": "Who won the world series in 2020?"},
    ],
)

cost = litellm.completion_cost(completion_response=response)
print(response.choices[0].message.content)
print(f"Custo da requisi√ß√£o: R$ {5.5 * cost:.2f}")


import tokencost

costs = tokencost.calculate_all_costs_and_tokens(
    prompt="Explique machine learning",
    completion="Machine learning √©...",
    model="gpt-4o-mini",
)

costs











import base64

from src.browser import set_browser, set_context, set_page
from src.clear_html import clean_html_for_llm

url = "https://descomplica.pocosdecaldas.mg.gov.br/"
# url = "https://resende.rj.gov.br/blogtransparencia/page/index.asp"

proxy_config = get_proxy()

assert await test_proxy(proxy_config)

async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="random", proxy=proxy_config)
    context = await set_context(browser)
    page = await set_page(context)
    await page.goto(url, wait_until="networkidle")
    html_content = await page.content()

    screenshot_bytes = await page.screenshot(full_page=True, type="jpeg", quality=40)

    screenshot_base64 = base64.b64encode(screenshot_bytes).decode("utf-8")


from src.prompts.homepage_check import check_homepage

response = await check_homepage(url, html_content)

import json

ResultadoBuscaServidores(**json.loads(response.choices[0].message.content))











from src.clear_html import clean_html_for_llm


cleaning_html = clean_html_for_llm(html_content, remove_classes=True)


len(cleaning_html["cleaned_html"])


# print(cleaning_html['cleaned_html'].replace("\n", ""))


from textwrap import dedent


def create_prompts(url, html_content, max_content_size: int = 50000):
    """
    Cria o prompt para P1: Identifica√ß√£o de Links Relevantes
    """

    system_prompt = dedent(
        """
    Voc√™ √© um especialista em an√°lise de portais de transpar√™ncia governamental brasileiros. 
    Sua miss√£o √© ajudar a construir um agente de IA para coletar dados salariais de servidores p√∫blicos municipais.

    INSTRU√á√ïES IMPORTANTES:
    1. Analise CUIDADOSAMENTE tanto a captura de tela quanto o c√≥digo HTML fornecido
    2. Procure por links, bot√µes ou menus que possam levar a informa√ß√µes sobre:
       - Transpar√™ncia
       - Servidores p√∫blicos
       - Folha de pagamento
       - Sal√°rios e remunera√ß√£o
       - Recursos humanos
       - Portal da transpar√™ncia
       - Gastos p√∫blicos
       - Despesas com pessoal

    3. Considere varia√ß√µes regionais nos termos (ex: "funcion√°rios", "colaboradores", "quadro de pessoal")
    4. Observe tanto elementos visuais √≥bvios quanto links menos evidentes no rodap√© ou menus secund√°rios
    5. Avalie a confian√ßa baseada na clareza e relev√¢ncia do link encontrado

    RESPONDA SEMPRE EM JSON V√ÅLIDO seguindo EXATAMENTE a estrutura especificada."""
    )

    user_prompt = dedent(
        f"""

    <main-question>
    Na p√°gina atual, existem links ou bot√µes que levam para informa√ß√µes sobre servidores p√∫blicos, transpar√™ncia, folha de pagamento ou sal√°rios?
    </main-question>

    URL ANALISADA: {url}

    CONTEXTO ADICIONAL:
    <context>
    - Esta √© a homepage de um portal municipal brasileiro
    - Estamos buscando o caminho para acessar dados de remunera√ß√£o de servidores p√∫blicos
    - Links podem estar em menus principais, rodap√©, ou se√ß√µes espec√≠ficas de transpar√™ncia
    </context>

    AN√ÅLISE REQUERIDA:
    <required-analysis>
    1. Examine a captura de tela para identificar elementos visuais relevantes
    2. Analise o HTML para encontrar links e estruturas de navega√ß√£o
    3. Identifique termos-chave relacionados √† transpar√™ncia e servidores p√∫blicos
    4. Determine a localiza√ß√£o e tipo de cada link encontrado
    5. Se o link encontrado for relativo, utilize a url da p√°gina para compor o link global
    </required-analysis>

    RESPONDA EM JSON SEGUINDO ESTA ESTRUTURA EXATA:
    <output-format>
    {{
      "tem_links_servidores": boolean,
      "links_encontrados": [
        {{
          "texto": "texto exato do link/bot√£o",
          "url": "URL relativa ou absoluta",
          "tipo": "link_principal|menu_dropdown|botao|rodape|sidebar|breadcrumb",
          "confianca": float_entre_0_e_1,
          "justificativa": "breve explica√ß√£o do por que este link √© relevante",
          "posicao_visual": "descri√ß√£o da localiza√ß√£o na p√°gina"
        }}
      ],
      "termos_identificados": ["lista", "de", "termos", "relevantes", "encontrados"],
      "localizacao_na_pagina": "menu_superior|sidebar|centro|rodape|multiplas_localizacoes",
      "observacoes_adicionais": "qualquer observa√ß√£o importante sobre a estrutura do site",
      "necessita_javascript": boolean,
      "nivel_dificuldade_navegacao": "facil|medio|dificil"
    }}
    </output-format>

    C√ìDIGO HTML DA P√ÅGINA:
    <page-html>
    {html_content[:max_content_size]}
    </page-html>

    <additional-important-information>
    IMPORTANTE: Se n√£o encontrar links √≥bvios, procure por:
    - Menus que possam ter submenus
    - Links no rodap√©
    - Se√ß√µes como "Acesso √† Informa√ß√£o" ou "LAI"
    - √çcones sem texto descritivo
    - Links em outras linguagens ou abrevia√ß√µes

    IMPORTANTE: os links relevantes deve come√ßar com `http`. Links relativos n√£o podem ser utilizados.
    </additional-important-information>
    """
    )

    return system_prompt, user_prompt


from enum import Enum
from typing import List, Literal, Optional

from pydantic import BaseModel, Field, HttpUrl


class TipoLink(str, Enum):
    """Tipos de links encontrados na p√°gina"""

    LINK_PRINCIPAL = "link_principal"
    MENU_DROPDOWN = "menu_dropdown"
    SUBMENU = "submenu"
    BOTAO = "botao"
    BANNER = "banner"


class LocalizacaoPagina(str, Enum):
    """Localiza√ß√£o dos elementos na p√°gina"""

    TOPO = "topo"
    CENTRO = "centro"
    RODAPE = "rodape"
    LATERAL = "lateral"


class NivelDificuldade(str, Enum):
    """N√≠vel de dificuldade para navega√ß√£o"""

    FACIL = "facil"
    MEDIO = "medio"
    DIFICIL = "dificil"


class LinkEncontrado(BaseModel):
    """Modelo para um link encontrado na p√°gina"""

    texto: str = Field(..., description="Texto do link")
    url: str = Field(..., description="URL do link (pode ser relativa ou absoluta)")
    tipo: TipoLink = Field(..., description="Tipo do link encontrado")
    confianca: float = Field(
        ..., ge=0.0, le=1.0, description="N√≠vel de confian√ßa (0.0 a 1.0)"
    )
    justificativa: str = Field(
        ..., description="Justificativa para o n√≠vel de confian√ßa"
    )
    posicao_visual: str = Field(
        ..., description="Descri√ß√£o da posi√ß√£o visual do link na p√°gina"
    )


class ResultadoBuscaServidores(BaseModel):
    """Modelo principal para resultado da busca de links de servidores p√∫blicos"""

    tem_links_servidores: bool = Field(
        ..., description="Indica se foram encontrados links relacionados a servidores"
    )
    links_encontrados: List[LinkEncontrado] = Field(
        default_factory=list, description="Lista de links encontrados"
    )
    termos_identificados: List[str] = Field(
        default_factory=list, description="Termos relevantes identificados na p√°gina"
    )
    localizacao_na_pagina: LocalizacaoPagina = Field(
        ..., description="Localiza√ß√£o principal dos links na p√°gina"
    )
    observacoes_adicionais: Optional[str] = Field(
        None, description="Observa√ß√µes adicionais sobre a busca"
    )
    necessita_javascript: bool = Field(
        False, description="Indica se √© necess√°rio JavaScript para acessar os links"
    )
    nivel_dificuldade_navegacao: NivelDificuldade = Field(
        ..., description="N√≠vel de dificuldade para navega√ß√£o"
    )

    class Config:
        """Configura√ß√£o do modelo"""

        use_enum_values = True
        validate_assignment = True


import litellm

MODEL = "gpt-4o-mini"
BRL_CURRENCY = 5.6


def check_homepage(url, html_content):

    cleaning_html = clean_html_for_llm(html_content, remove_classes=True)

    system_prompt, user_prompt = create_prompts(
        url, cleaning_html["cleaned_html"].replace("\n", "")
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    response = await litellm.acompletion(
        model=MODEL,
        response_format=ResultadoBuscaServidores,
        messages=messages,
    )

    cost = litellm.completion_cost(model=MODEL, completion_response=response)

    setattr(response, "cost_usd", cost)
    setattr(response, "cost_brl", cost * BRL_CURRENCY)

    return response


setattr(response, "cost", 1)


response








import litellm

MODEL = "gpt-4o-mini"


async def check_homepage():

    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        {"role": "user", "content": user_prompt},
    ]

    response = await litellm.acompletion(
        model=MODEL,
        response_format={"type": "json_object"},
        messages=messages,
    )

    cost = litellm.completion_cost(model=model, completion_response=response)
    print(response.choices[0].message.content)
    print(f"Custo da requisi√ß√£o: R$ {5.5 * cost:.4f}")


import litellm

model = "gpt-4o"
messages = [
    {
        "role": "system",
        "content": system_prompt,
    },
    {"role": "user", "content": user_prompt},
]

response = await litellm.acompletion(
    model=model,
    response_format={"type": "json_object"},
    messages=messages,
)

cost = litellm.completion_cost(model=model, completion_response=response)
print(response.choices[0].message.content)
print(f"Custo da requisi√ß√£o: R$ {5.5 * cost:.4f}")






































5.5 * litellm.completion_cost(model=model, completion_response=response, messages=[])


response


# from openai import OpenAI

# # Configurar cliente
# client = OpenAI(
#     api_key=os.environ["DEEPSEEK_API_KEY"],
#     base_url="https://api.deepseek.com"
# )


# response = client.chat.completions.parse(
#     model="deepseek-chat",
#     messages=[
#         {"role": "system", "content": "system_prompt"},
#         {"role": "user", "content": "user_prompt"},
#     ],
#     response_format=ResultadoBuscaServidores,
# )

# # Acessar o modelo parseado
# resultado = response.choices[0].message.parsed
# print("Modelo parseado com sucesso!")
# print(f"Encontrados {len(resultado.links_encontrados)} links")
# print(f"N√≠vel de dificuldade: {resultado.nivel_dificuldade_navegacao}")


with open("teste.html", "w") as f:
    f.write(html_content)


# print(user_prompt)





import litellm

response = await litellm.acompletion(
    model="deepseek/deepseek-chat",
    response_format={"type": "json_object"},
    messages=[
        {
            "role": "system",
            "content": system_prompt,
        },
        {"role": "user", "content": user_prompt},
    ],
)

cost = litellm.completion_cost(completion_response=response)
print(response.choices[0].message.content)
print(f"Custo da requisi√ß√£o: R$ {5.5 * cost:.4f}")





cost


import tokencost

costs = tokencost.calculate_all_costs_and_tokens(
    prompt=system_prompt + user_prompt,
    completion=response.choices[0].message.content,
    model="gpt-4o-mini",
    a,
)

costs




















# show_base64(screenshot_base64)


from pydantic import BaseModel

city = "Po√ßos de Caldas"
uf = "MG"

history = [
    {
        "type": "contexto",
        "message": (f"O usu√°rio j√© se apresentou a voc√™ lhe informou o seu objetivo"),
    },
    {
        "type": "contexto",
        "message": "O usu√°rio ja lhe forneceu a url da p√°gina principal",
    },
    {
        "type": "seu pensamento",
        "message": '"Preciso definir se tem algum link interessante nessa p√°gina que me leve a uma outra p√°gina onde terei a tabela com a rela√ß√£o nominal de sal√°rio. "',
    },
]


formatted_history = "\n".join(
    [f"Step {i+1}: [{x['type']}] {x['message']}" for i, x in enumerate(history)]
)

prompt = f"""
<objetivo>
Encontrar uma forma de obter o sal√°rio nominal de cada uma dos servidores p√∫blicos da cidade de {city} - {uf}
</objetivo>

<sua-tarefa>
Verfique no html da p√°gina se existem links relevantes que leval a outras p√°ginas onde possivelmente
poderei encontrar a rela√ß√£o entre nome e sal√°rio dos servidores p√∫blicos da cidade de {city} - {uf}.
</sua-tarefa>

<html-da-pagina>
{html}
</html-da-pagina>

<history>
{formatted_history}
</history>
"""


class LLMCompletion(BaseModel):
    relevant_links: list[str]


# print(prompt)


costs = tokencost.calculate_all_costs_and_tokens(
    prompt=formatted_history, completion="", model="gpt-4o-mini"
)

costs





import litellm

response = await litellm.acompletion(
    model="gpt-4o-mini",
    response_format=LLMCompletion,
    messages=[
        {
            "role": "system",
            "content": "Voc√™ √© um assistente especialista em navega√ß√£o web.",
        },
        {"role": "user", "content": prompt},
    ],
)

cost = litellm.completion_cost(completion_response=response)
print(response.choices[0].message.content)
print(f"Custo da requisi√ß√£o: R$ {5.5 * cost:.2f}")


url = "https://descomplica.pocosdecaldas.mg.gov.br"

proxy_config = get_proxy()

assert await test_proxy(proxy_config)

async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="random", proxy=proxy_config)
    context = await set_context(browser)
    page = await set_page(context)
    await page.goto(url, wait_until="networkidle")
    html = await page.content()

response = await litellm.acompletion(
    model="gpt-4o-mini",
    response_format=LLMCompletion,
    messages=[
        {
            "role": "system",
            "content": "Voc√™ √© um assistente especialista em navega√ß√£o web.",
        },
        {"role": "user", "content": prompt},
    ],
)

cost = litellm.completion_cost(completion_response=response)
print(response.choices[0].message.content)
print(f"Custo da requisi√ß√£o: R$ {5.5 * cost:.2f}")



































# print(html)


proxy_config = get_proxy()
print_proxy_safe(proxy_config)








import os


from openai import OpenAI

client = OpenAI(
    api_key=os.environ["DEEPSEEK_API_KEY"], base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    stream=False,
)

print(response.choices[0].message.content)

















get_ipython().getoutput("pip install browser-use -qqq")


import asyncio
import os

from browser_use import Agent
from browser_use.llm import ChatOpenAI

llm = ChatOpenAI(
    base_url="https://api.deepseek.com/v1",  # ou sua inst√¢ncia local
    api_key=os.environ["DEEPSEEK_API_KEY"],
    model="deepseek-v3",
)

# llm = ChatOpenAI(
#     api_key=os.environ["OPENAI_API_KEY"],
#     model="gpt-4o-mini",
# )

agent = Agent(
    task="encontre o link para a pagia onde eu consiga encontrar o sal√°rio dos servidores p√∫blicos municipais da cidade de resende-rj",
    llm=llm,
)
await agent.run()











import asyncio
import json

from browser_use import Agent
from browser_use.llm import ChatOpenAI

llm = ChatOpenAI(
    base_url="https://api.deepseek.com/v1",  # ou sua inst√¢ncia local
    api_key=chave_deepseek,
    model="deepseek-v3",
)

agent = Agent(
    task="""
    Acesse o Google, busque por "melhores frameworks Python para IA em 2025".
    Extraia os 5 primeiros resultados com:
    - T√≠tulo da p√°gina
    - Link da p√°gina
    - Um pequeno resumo (1-2 frases)

    Formate o resultado assim:
    [
      {
        "title": "T√≠tulo da p√°gina",
        "url": "https://...",
        "summary": "Resumo do conte√∫do"
      },
      ...
    ]
    S√≥ responda com o JSON. N√£o inclua nenhuma explica√ß√£o.
    """,
    llm=llm,
)

result = await agent.run()
print("RESULTADO:\n")
try:
    parsed = json.loads(result)
    for i, item in enumerate(parsed, 1):
        print(f"{i}. {item['title']}")
        print(f"   URL: {item['url']}")
        print(f"   RESUMO: {item['summary']}\n")
except Exception as e:
    print("N√£o foi poss√≠vel converter para JSON:\n")
    print(result)





import asyncio
import json

from browser_use import Agent
from browser_use.llm import ChatOpenAI


async def main():
    agent = Agent(
        task="""
        Acesse o Google, busque por "melhores frameworks Python para IA em 2025".
        Extraia os 5 primeiros resultados com:
        - T√≠tulo da p√°gina
        - Link da p√°gina
        - Um pequeno resumo (1-2 frases)

        Formate o resultado assim:
        [
          {
            "title": "T√≠tulo da p√°gina",
            "url": "https://...",
            "summary": "Resumo do conte√∫do"
          },
          ...
        ]
        S√≥ responda com o JSON. N√£o inclua nenhuma explica√ß√£o.
        """,
        llm=ChatOpenAI(model="gpt-4o", temperature=0.3),
    )

    result = await agent.run()
    print("RESULTADO:\n")
    try:
        parsed = json.loads(result)
        for i, item in enumerate(parsed, 1):
            print(f"{i}. {item['title']}")
            print(f"   URL: {item['url']}")
            print(f"   RESUMO: {item['summary']}\n")
    except Exception as e:
        print("N√£o foi poss√≠vel converter para JSON:\n")
        print(result)


asyncio.run(main())





page


url = "https://www.bing.com/search?q=portal+de+transparencia+resende"

async with async_playwright() as playwright:
    browser = await set_browser(playwright, engine="random")
    context = await set_context(browser)
    page = await set_page(context)

    try:
        print("Navegar para o Google")
        await page.goto(url, wait_until="networkidle")

        print("Pegar conte√∫do da p√°gina")
        html_page = await page.content()

        # print("Aguardar o campo de busca aparecer e clicar nele")
        # search_input = page.locator('input[name="q"]')
        # await search_input.wait_for(state="visible")

        # print("Digitar a consulta")
        # await search_input.fill(query)

        # print("Pressionar Enter ou clicar no bot√£o de busca")
        # await search_input.press("Enter")

        # print("Aguardar os resultados carregarem")
        # await page.wait_for_selector('#search', timeout=10000)

        # print("Aguardar um pouco mais para garantir que todos os resultados carregaram")
        # await page.wait_for_timeout(2000)

        # print("Coletar os resultados de busca")

    except Exception as e:
        print(f"Erro durante a busca: {e}")

    finally:
        await browser.close()


html_page

















# T√âCNICAS AVAN√áADAS PARA EVITAR DETEC√á√ÉO

# 1. HUMANIZA√á√ÉO EXTREMA
import asyncio
import math
import random


async def human_mouse_movement(page, start_x, start_y, end_x, end_y):
    """Simula movimento de mouse humano com curvas naturais"""

    steps = random.randint(15, 25)

    for i in range(steps):
        progress = i / steps

        # Curva B√©zier para movimento natural
        control_x = start_x + random.uniform(-50, 50)
        control_y = start_y + random.uniform(-50, 50)

        # Calcular posi√ß√£o atual
        current_x = (
            (1 - progress) ** 2 * start_x
            + 2 * (1 - progress) * progress * control_x
            + progress**2 * end_x
        )
        current_y = (
            (1 - progress) ** 2 * start_y
            + 2 * (1 - progress) * progress * control_y
            + progress**2 * end_y
        )

        # Adicionar pequenas varia√ß√µes
        current_x += random.uniform(-2, 2)
        current_y += random.uniform(-2, 2)

        await page.mouse.move(current_x, current_y)
        await asyncio.sleep(random.uniform(0.01, 0.03))


async def human_typing(page, element, text):
    """Digita√ß√£o com padr√µes humanos realistas"""

    await element.click()
    await asyncio.sleep(random.uniform(0.1, 0.3))

    for i, char in enumerate(text):
        # Velocidade vari√°vel (mais lento no in√≠cio, mais r√°pido depois)
        base_delay = 0.12 if i < 3 else 0.08

        # Caracteres especiais demoram mais
        if char in " -_@.":
            delay = base_delay * random.uniform(1.5, 2.5)
        else:
            delay = base_delay * random.uniform(0.7, 1.3)

        # Pequena chance de pausar (pensando)
        if random.random() < 0.05:
            delay += random.uniform(0.5, 1.5)

        await element.type(char)
        await asyncio.sleep(delay)


# 2. BROWSER FINGERPRINTING AVAN√áADO
async def setup_realistic_browser(playwright):
    """Configura browser com fingerprint extremamente realista"""

    # Fingerprints reais coletados de browsers verdadeiros
    realistic_configs = [
        {
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "viewport": {"width": 1920, "height": 1080},
            "screen": {"width": 1920, "height": 1080},
            "timezone": "America/Sao_Paulo",
            "locale": "pt-BR",
            "platform": "Win32",
        },
        {
            "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "viewport": {"width": 1440, "height": 900},
            "screen": {"width": 1440, "height": 900},
            "timezone": "America/Sao_Paulo",
            "locale": "pt-BR",
            "platform": "MacIntel",
        },
    ]

    config = random.choice(realistic_configs)

    browser = await playwright.chromium.launch(
        headless=True,
        args=[
            "--no-sandbox",
            "--disable-setuid-sandbox",
            "--disable-dev-shm-usage",
            "--disable-blink-features=AutomationControlled",
            "--exclude-switches=enable-automation",
            "--disable-extensions",
            "--disable-plugins-discovery",
            "--disable-plugins",
            "--disable-preconnect",
            "--disable-gpu",
            "--no-first-run",
            "--no-service-autorun",
            "--password-store=basic",
            "--system-developer-mode",
            "--mute-audio",
            "--no-zygote",
            "--no-default-browser-check",
            "--disable-backgrounding-occluded-windows",
            "--disable-renderer-backgrounding",
            "--disable-features=TranslateUI",
            "--disable-ipc-flooding-protection",
            f'--window-size={config["viewport"]["width"]},{config["viewport"]["height"]}',
        ],
    )

    context = await browser.new_context(
        user_agent=config["user_agent"],
        viewport=config["viewport"],
        screen=config["screen"],
        locale=config["locale"],
        timezone_id=config["timezone"],
        permissions=["geolocation"],
        geolocation={"latitude": -22.4609, "longitude": -44.4444},  # Resende, RJ
        extra_http_headers={
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7",
            "Cache-Control": "max-age=0",
            "Sec-Ch-Ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": f'"{config["platform"]}"',
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1",
        },
    )

    return browser, context


# 3. STEALTH SCRIPTS AVAN√áADOS
ADVANCED_STEALTH_SCRIPT = """
(() => {
    // 1. Remover propriedades de automa√ß√£o
    delete navigator.__proto__.webdriver;
    delete navigator.webdriver;
    
    // 2. Mockear propriedades ausentes
    Object.defineProperty(navigator, 'webdriver', {
        get: () => undefined,
        configurable: true
    });
    
    // 3. Simular plugins reais
    Object.defineProperty(navigator, 'plugins', {
        get: () => [
            {
                0: {type: "application/x-google-chrome-pdf", suffixes: "pdf", description: "Portable Document Format", enabledPlugin: Plugin},
                description: "Portable Document Format",
                filename: "internal-pdf-viewer",
                length: 1,
                name: "Chrome PDF Plugin"
            },
            {
                0: {type: "application/pdf", suffixes: "pdf", description: "Portable Document Format", enabledPlugin: Plugin},
                description: "Portable Document Format", 
                filename: "mhjfbmdgcfjbbpaeojofohoefgiehjai",
                length: 1,
                name: "Chrome PDF Viewer"
            }
        ]
    });
    
    // 4. Simular propriedades de hardware
    Object.defineProperty(navigator, 'hardwareConcurrency', {
        get: () => 8
    });
    
    Object.defineProperty(navigator, 'deviceMemory', {
        get: () => 8
    });
    
    // 5. Mockear WebGL fingerprint
    const getParameter = WebGLRenderingContext.getParameter;
    WebGLRenderingContext.prototype.getParameter = function(parameter) {
        if (parameter === 37445) {
            return 'NVIDIA Corporation';
        }
        if (parameter === 37446) {
            return 'NVIDIA GeForce GTX 1060 6GB/PCIe/SSE2';
        }
        return getParameter(parameter);
    };
    
    // 6. Mockear screen properties
    Object.defineProperty(screen, 'availWidth', {
        get: () => window.screen.width
    });
    
    Object.defineProperty(screen, 'availHeight', {
        get: () => window.screen.height - 40  // Taskbar
    });
    
    // 7. Adicionar propriedades do Chrome ausentes
    if (!window.chrome) {
        window.chrome = {
            app: {
                isInstalled: false,
                InstallState: {
                    DISABLED: 'disabled',
                    INSTALLED: 'installed',
                    NOT_INSTALLED: 'not_installed'
                },
                RunningState: {
                    CANNOT_RUN: 'cannot_run',
                    READY_TO_RUN: 'ready_to_run',
                    RUNNING: 'running'
                }
            },
            runtime: {
                OnInstalledReason: {
                    CHROME_UPDATE: 'chrome_update',
                    INSTALL: 'install',
                    SHARED_MODULE_UPDATE: 'shared_module_update',
                    UPDATE: 'update'
                },
                OnRestartRequiredReason: {
                    APP_UPDATE: 'app_update',
                    OS_UPDATE: 'os_update',
                    PERIODIC: 'periodic'
                },
                PlatformArch: {
                    ARM: 'arm',
                    ARM64: 'arm64',
                    MIPS: 'mips',
                    MIPS64: 'mips64',
                    X86_32: 'x86-32',
                    X86_64: 'x86-64'
                },
                PlatformNaclArch: {
                    ARM: 'arm',
                    MIPS: 'mips',
                    MIPS64: 'mips64',
                    X86_32: 'x86-32',
                    X86_64: 'x86-64'
                },
                PlatformOs: {
                    ANDROID: 'android',
                    CROS: 'cros',
                    FREEBSD: 'freebsd',
                    LINUX: 'linux',
                    MAC: 'mac',
                    OPENBSD: 'openbsd',
                    WIN: 'win'
                },
                RequestUpdateCheckStatus: {
                    NO_UPDATE: 'no_update',
                    THROTTLED: 'throttled',
                    UPDATE_AVAILABLE: 'update_available'
                }
            }
        };
    }
    
    // 8. Remover automation flags
    Object.defineProperty(navigator, 'languages', {
        get: () => ['pt-BR', 'pt', 'en-US', 'en']
    });
    
    // 9. Simular permissions reais
    const originalQuery = window.navigator.permissions.query;
    window.navigator.permissions.query = (parameters) => (
        parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission }) :
            originalQuery(parameters)
    );
    
    // 10. Adicionar event listeners naturais
    ['mousedown', 'mouseup', 'click', 'mousemove'].forEach(eventType => {
        document.addEventListener(eventType, () => {}, true);
    });
    
})();
"""


# 4. FUN√á√ÉO PRINCIPAL COM TODAS AS T√âCNICAS
async def ultimate_stealth_search(query):
    """Busca com t√©cnicas stealth de √∫ltima gera√ß√£o"""

    async with async_playwright() as playwright:
        browser, context = await setup_realistic_browser(playwright)
        page = await context.new_page()

        # Aplicar script stealth
        await page.add_init_script(ADVANCED_STEALTH_SCRIPT)

        try:
            print("üïµÔ∏è Iniciando busca stealth de √∫ltima gera√ß√£o...")

            # Navegar com comportamento humano
            await page.goto("https://www.google.com.br")
            await asyncio.sleep(random.uniform(2, 4))

            # Simular comportamento de usu√°rio real
            await page.mouse.move(100, 100)
            await asyncio.sleep(0.5)

            # Scroll para baixo e para cima (comportamento natural)
            await page.mouse.wheel(0, 200)
            await asyncio.sleep(0.5)
            await page.mouse.wheel(0, -100)
            await asyncio.sleep(1)

            # Encontrar campo de busca
            search_box = page.locator('input[name="q"]').first
            await search_box.wait_for(state="visible")

            # Movimento natural do mouse at√© o campo
            box = await search_box.bounding_box()
            if box:
                await human_mouse_movement(
                    page,
                    random.randint(100, 300),
                    random.randint(100, 300),
                    box["x"] + box["width"] / 2,
                    box["y"] + box["height"] / 2,
                )

            # Digita√ß√£o humana
            await human_typing(page, search_box, query)

            # Pausa antes de pressionar Enter
            await asyncio.sleep(random.uniform(0.5, 1.5))
            await search_box.press("Enter")

            # Aguardar com comportamento natural
            await asyncio.sleep(random.uniform(3, 6))

            # Verificar se chegaram resultados
            try:
                await page.wait_for_selector(".g, #search", timeout=10000)
                print("‚úÖ Resultados carregados com sucesso!")

                # Coletar resultados (mesmo c√≥digo anterior)
                results = []
                search_results = await page.query_selector_all(".g")

                for i, result in enumerate(search_results[:10]):
                    try:
                        title_el = await result.query_selector("h3")
                        title = (
                            await title_el.inner_text() if title_el else "Sem t√≠tulo"
                        )

                        url_el = await result.query_selector("a")
                        url = (
                            await url_el.get_attribute("href") if url_el else "Sem URL"
                        )

                        snippet_el = await result.query_selector(".VwiC3b, .s3v9rd")
                        snippet = (
                            await snippet_el.inner_text()
                            if snippet_el
                            else "Sem snippet"
                        )

                        if title != "Sem t√≠tulo" and url != "Sem URL":
                            results.append(
                                {
                                    "position": i + 1,
                                    "title": title.strip(),
                                    "url": url,
                                    "snippet": snippet.strip(),
                                }
                            )

                    except Exception:
                        continue

                return results

            except:
                # Verificar se foi bloqueado
                captcha = await page.query_selector("#recaptcha, .g-recaptcha")
                if captcha:
                    print("‚ùå Ainda detectado como bot")
                else:
                    print("‚ùå Timeout nos resultados")
                return None

        finally:
            await browser.close()


# 5. EXEMPLO DE USO
async def main():
    query = "portal de transpar√™ncia de Resende - RJ"
    results = await ultimate_stealth_search(query)

    if results:
        print(f"\nüéâ SUCESSO! {len(results)} resultados obtidos")
        for r in results[:3]:
            print(f"‚Ä¢ {r['title']}")
    else:
        print("‚ùå Ainda n√£o foi poss√≠vel burlar a detec√ß√£o")


if __name__ == "__main__":
    asyncio.run(main())









